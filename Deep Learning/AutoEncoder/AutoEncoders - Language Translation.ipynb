{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94ec8666",
   "metadata": {},
   "source": [
    "# Machine Learning Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd793179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size=64 #Batch size for training\n",
    "epochs=100 #No of epochs to train for\n",
    "latent_dim=256 #Latent dimensionality of the encoding space\n",
    "num_samples=10000 #no of samples to train on\n",
    "#path to the data txt file on disk\n",
    "data_path=r\"C:\\Users\\ShibuKumar\\Downloads\\Datas\\fra.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9cb5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize the data\n",
    "input_texts=[]\n",
    "target_texts=[]\n",
    "input_characters=set()\n",
    "target_characters=set()\n",
    "with open(data_path,\"r\",encoding=\"utf-8\") as f:\n",
    "    lines=f.read().split(\"\\n\")\n",
    "for line in lines[:min(num_samples,len(lines)-1)]:\n",
    "    input_text,target_text=line.split(\"\\t\")\n",
    "    #we use \"tab\" as the \"start sequence\" character.\n",
    "    #for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text=\"\\t\"+target_text+\"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01d01481",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters=sorted(list(input_characters))\n",
    "target_characters=sorted(list(target_characters))\n",
    "num_encoder_tokens=len(input_characters)\n",
    "num_decoder_tokens=len(target_characters)\n",
    "max_encoder_seq_length=max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length=max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b2ed63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 94\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Samples:\",len(input_texts))\n",
    "print(\"Number of unique input tokens:\",num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\",num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\",max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\",max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2dafacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index=dict([\n",
    "    (char,i) for i,char in enumerate(input_characters)\n",
    "])\n",
    "target_token_index=dict([\n",
    "    (char,i) for i,char in enumerate(target_characters)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26bb0741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({' ': 0,\n",
       "  '!': 1,\n",
       "  '$': 2,\n",
       "  '%': 3,\n",
       "  '&': 4,\n",
       "  \"'\": 5,\n",
       "  ',': 6,\n",
       "  '-': 7,\n",
       "  '.': 8,\n",
       "  '0': 9,\n",
       "  '1': 10,\n",
       "  '2': 11,\n",
       "  '3': 12,\n",
       "  '4': 13,\n",
       "  '5': 14,\n",
       "  '6': 15,\n",
       "  '7': 16,\n",
       "  '8': 17,\n",
       "  '9': 18,\n",
       "  ':': 19,\n",
       "  '?': 20,\n",
       "  'A': 21,\n",
       "  'B': 22,\n",
       "  'C': 23,\n",
       "  'D': 24,\n",
       "  'E': 25,\n",
       "  'F': 26,\n",
       "  'G': 27,\n",
       "  'H': 28,\n",
       "  'I': 29,\n",
       "  'J': 30,\n",
       "  'K': 31,\n",
       "  'L': 32,\n",
       "  'M': 33,\n",
       "  'N': 34,\n",
       "  'O': 35,\n",
       "  'P': 36,\n",
       "  'Q': 37,\n",
       "  'R': 38,\n",
       "  'S': 39,\n",
       "  'T': 40,\n",
       "  'U': 41,\n",
       "  'V': 42,\n",
       "  'W': 43,\n",
       "  'Y': 44,\n",
       "  'a': 45,\n",
       "  'b': 46,\n",
       "  'c': 47,\n",
       "  'd': 48,\n",
       "  'e': 49,\n",
       "  'f': 50,\n",
       "  'g': 51,\n",
       "  'h': 52,\n",
       "  'i': 53,\n",
       "  'j': 54,\n",
       "  'k': 55,\n",
       "  'l': 56,\n",
       "  'm': 57,\n",
       "  'n': 58,\n",
       "  'o': 59,\n",
       "  'p': 60,\n",
       "  'q': 61,\n",
       "  'r': 62,\n",
       "  's': 63,\n",
       "  't': 64,\n",
       "  'u': 65,\n",
       "  'v': 66,\n",
       "  'w': 67,\n",
       "  'x': 68,\n",
       "  'y': 69,\n",
       "  'z': 70},\n",
       " {'\\t': 0,\n",
       "  '\\n': 1,\n",
       "  ' ': 2,\n",
       "  '!': 3,\n",
       "  '$': 4,\n",
       "  '%': 5,\n",
       "  '&': 6,\n",
       "  \"'\": 7,\n",
       "  '(': 8,\n",
       "  ')': 9,\n",
       "  ',': 10,\n",
       "  '-': 11,\n",
       "  '.': 12,\n",
       "  '0': 13,\n",
       "  '1': 14,\n",
       "  '3': 15,\n",
       "  '5': 16,\n",
       "  '6': 17,\n",
       "  '8': 18,\n",
       "  '9': 19,\n",
       "  ':': 20,\n",
       "  '?': 21,\n",
       "  'A': 22,\n",
       "  'B': 23,\n",
       "  'C': 24,\n",
       "  'D': 25,\n",
       "  'E': 26,\n",
       "  'F': 27,\n",
       "  'G': 28,\n",
       "  'H': 29,\n",
       "  'I': 30,\n",
       "  'J': 31,\n",
       "  'K': 32,\n",
       "  'L': 33,\n",
       "  'M': 34,\n",
       "  'N': 35,\n",
       "  'O': 36,\n",
       "  'P': 37,\n",
       "  'Q': 38,\n",
       "  'R': 39,\n",
       "  'S': 40,\n",
       "  'T': 41,\n",
       "  'U': 42,\n",
       "  'V': 43,\n",
       "  'Y': 44,\n",
       "  'a': 45,\n",
       "  'b': 46,\n",
       "  'c': 47,\n",
       "  'd': 48,\n",
       "  'e': 49,\n",
       "  'f': 50,\n",
       "  'g': 51,\n",
       "  'h': 52,\n",
       "  'i': 53,\n",
       "  'j': 54,\n",
       "  'k': 55,\n",
       "  'l': 56,\n",
       "  'm': 57,\n",
       "  'n': 58,\n",
       "  'o': 59,\n",
       "  'p': 60,\n",
       "  'q': 61,\n",
       "  'r': 62,\n",
       "  's': 63,\n",
       "  't': 64,\n",
       "  'u': 65,\n",
       "  'v': 66,\n",
       "  'w': 67,\n",
       "  'x': 68,\n",
       "  'y': 69,\n",
       "  'z': 70,\n",
       "  '\\xa0': 71,\n",
       "  '«': 72,\n",
       "  '»': 73,\n",
       "  'À': 74,\n",
       "  'Ç': 75,\n",
       "  'É': 76,\n",
       "  'Ê': 77,\n",
       "  'à': 78,\n",
       "  'â': 79,\n",
       "  'ç': 80,\n",
       "  'è': 81,\n",
       "  'é': 82,\n",
       "  'ê': 83,\n",
       "  'ë': 84,\n",
       "  'î': 85,\n",
       "  'ï': 86,\n",
       "  'ô': 87,\n",
       "  'ù': 88,\n",
       "  'û': 89,\n",
       "  'œ': 90,\n",
       "  '\\u2009': 91,\n",
       "  '’': 92,\n",
       "  '\\u202f': 93})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index, target_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cdbc56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data=np.zeros(\n",
    "    (len(input_texts),max_encoder_seq_length,num_encoder_tokens),\n",
    "    dtype=\"float32\")\n",
    "decoder_input_data=np.zeros(\n",
    "    (len(input_texts),max_decoder_seq_length,num_decoder_tokens),\n",
    "    dtype=\"float32\")\n",
    "decoder_target_data=np.zeros(\n",
    "    (len(input_texts,),max_decoder_seq_length,num_decoder_tokens),\n",
    "    dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2cc0acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot representation\n",
    "for i,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n",
    "    for t,char in enumerate(input_text):\n",
    "        encoder_input_data[i,t,input_token_index[char]]=1.\n",
    "    encoder_input_data[i,t+1:,input_token_index[\" \"]]=1.\n",
    "    for t,char in enumerate(target_text):\n",
    "        #decoder target data is ahead of decoder_input_data by one time step\n",
    "        decoder_input_data[i,t,target_token_index[char]]=1.\n",
    "        if t>0:\n",
    "            #decoder_target_data will be ahead by one timestep\n",
    "            #and will not include the start character.\n",
    "            decoder_target_data[i,t-1,target_token_index[char]]=1.\n",
    "    decoder_input_data[i,t+1:,target_token_index[\" \"]]=1.\n",
    "    decoder_target_data[i,t:,target_token_index[\" \"]]=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b0916e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 71)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9f8767ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define an input sequence and process it.\n",
    "encoder_inputs=Input(shape=(None,num_encoder_tokens))\n",
    "encoder=LSTM(latent_dim,return_state=True)\n",
    "encoder_outputs,state_h,state_c=encoder(encoder_inputs)\n",
    "#we discard \"encoder_outputs\" and only keep the states\n",
    "encoder_states=[state_h,state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5955bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the decoder,using \"encoder_states\" as inital state.\n",
    "decoder_inputs=Input(shape=(None,num_decoder_tokens))\n",
    "#we set up our decoder to return full output sequence,\n",
    "#and to return internal states as well. we don\"t use the\n",
    "#return states in the training model, but we will use them in inference.\n",
    "decoder_lstm=LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "decoder_outputs,_,_=decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "decoder_dense=Dense(num_decoder_tokens,activation=\"softmax\")\n",
    "decoder_outputs=decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4047be43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ShibuKumar\\AppData\\Local\\miniconda3\\envs\\tfdirectml\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 61s 8ms/sample - loss: 1.2105 - acc: 0.7208 - val_loss: 1.1300 - val_acc: 0.7012\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 59s 7ms/sample - loss: 0.8636 - acc: 0.7661 - val_loss: 0.8896 - val_acc: 0.7560\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 57s 7ms/sample - loss: 0.6947 - acc: 0.8058 - val_loss: 0.7582 - val_acc: 0.7788\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.5972 - acc: 0.8264 - val_loss: 0.6819 - val_acc: 0.7991\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.5492 - acc: 0.8393 - val_loss: 0.6489 - val_acc: 0.8082\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.5116 - acc: 0.8493 - val_loss: 0.6121 - val_acc: 0.8176\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.4826 - acc: 0.8573 - val_loss: 0.5846 - val_acc: 0.8247\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 56s 7ms/sample - loss: 0.4578 - acc: 0.8640 - val_loss: 0.5649 - val_acc: 0.8324\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.4358 - acc: 0.8700 - val_loss: 0.5492 - val_acc: 0.8356\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 56s 7ms/sample - loss: 0.4160 - acc: 0.8758 - val_loss: 0.5295 - val_acc: 0.8417\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.3984 - acc: 0.8808 - val_loss: 0.5189 - val_acc: 0.8462\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 56s 7ms/sample - loss: 0.3824 - acc: 0.8850 - val_loss: 0.5089 - val_acc: 0.8484\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.3674 - acc: 0.8895 - val_loss: 0.5020 - val_acc: 0.8511\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 59s 7ms/sample - loss: 0.3537 - acc: 0.8935 - val_loss: 0.4949 - val_acc: 0.8532\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 61s 8ms/sample - loss: 0.3409 - acc: 0.8973 - val_loss: 0.4882 - val_acc: 0.8554\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 61s 8ms/sample - loss: 0.3282 - acc: 0.9012 - val_loss: 0.4807 - val_acc: 0.8583\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 59s 7ms/sample - loss: 0.3170 - acc: 0.9046 - val_loss: 0.4796 - val_acc: 0.8590\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 58s 7ms/sample - loss: 0.3055 - acc: 0.9079 - val_loss: 0.4797 - val_acc: 0.8600\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 58s 7ms/sample - loss: 0.2953 - acc: 0.9109 - val_loss: 0.4747 - val_acc: 0.8616\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 59s 7ms/sample - loss: 0.2853 - acc: 0.9139 - val_loss: 0.4754 - val_acc: 0.8615\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 58s 7ms/sample - loss: 0.2757 - acc: 0.9166 - val_loss: 0.4736 - val_acc: 0.8640\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 57s 7ms/sample - loss: 0.2667 - acc: 0.9193 - val_loss: 0.4755 - val_acc: 0.8635\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 57s 7ms/sample - loss: 0.2581 - acc: 0.9221 - val_loss: 0.4711 - val_acc: 0.8644\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 57s 7ms/sample - loss: 0.2499 - acc: 0.9244 - val_loss: 0.4719 - val_acc: 0.8659\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 57s 7ms/sample - loss: 0.2413 - acc: 0.9271 - val_loss: 0.4777 - val_acc: 0.8646\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 59s 7ms/sample - loss: 0.2343 - acc: 0.9289 - val_loss: 0.4790 - val_acc: 0.8651\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 57s 7ms/sample - loss: 0.2266 - acc: 0.9316 - val_loss: 0.4796 - val_acc: 0.8666\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 57s 7ms/sample - loss: 0.2193 - acc: 0.9334 - val_loss: 0.4822 - val_acc: 0.8662\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 57s 7ms/sample - loss: 0.2128 - acc: 0.9354 - val_loss: 0.4860 - val_acc: 0.8660\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 57s 7ms/sample - loss: 0.2065 - acc: 0.9373 - val_loss: 0.4883 - val_acc: 0.8663\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 57s 7ms/sample - loss: 0.2000 - acc: 0.9390 - val_loss: 0.4937 - val_acc: 0.8663\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 58s 7ms/sample - loss: 0.1942 - acc: 0.9411 - val_loss: 0.4970 - val_acc: 0.8675\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 65s 8ms/sample - loss: 0.1887 - acc: 0.9424 - val_loss: 0.4991 - val_acc: 0.8672\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 65s 8ms/sample - loss: 0.1831 - acc: 0.9445 - val_loss: 0.5031 - val_acc: 0.8666\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 58s 7ms/sample - loss: 0.1782 - acc: 0.9457 - val_loss: 0.5075 - val_acc: 0.8669\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 58s 7ms/sample - loss: 0.1729 - acc: 0.9473 - val_loss: 0.5145 - val_acc: 0.8666\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 58s 7ms/sample - loss: 0.1680 - acc: 0.9487 - val_loss: 0.5184 - val_acc: 0.8654\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 58s 7ms/sample - loss: 0.1636 - acc: 0.9497 - val_loss: 0.5220 - val_acc: 0.8662\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 57s 7ms/sample - loss: 0.1591 - acc: 0.9517 - val_loss: 0.5264 - val_acc: 0.8667\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 58s 7ms/sample - loss: 0.1549 - acc: 0.9529 - val_loss: 0.5273 - val_acc: 0.8676\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 62s 8ms/sample - loss: 0.1507 - acc: 0.9538 - val_loss: 0.5402 - val_acc: 0.8648\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 59s 7ms/sample - loss: 0.1465 - acc: 0.9552 - val_loss: 0.5451 - val_acc: 0.8655\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 58s 7ms/sample - loss: 0.1429 - acc: 0.9563 - val_loss: 0.5486 - val_acc: 0.8659\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 58s 7ms/sample - loss: 0.1393 - acc: 0.9572 - val_loss: 0.5543 - val_acc: 0.8661\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.1359 - acc: 0.9580 - val_loss: 0.5550 - val_acc: 0.8671\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 193s 24ms/sample - loss: 0.1323 - acc: 0.9593 - val_loss: 0.5653 - val_acc: 0.8659\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 56s 7ms/sample - loss: 0.1291 - acc: 0.9602 - val_loss: 0.5662 - val_acc: 0.8658\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.1263 - acc: 0.9611 - val_loss: 0.5710 - val_acc: 0.8663\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 56s 7ms/sample - loss: 0.1232 - acc: 0.9618 - val_loss: 0.5736 - val_acc: 0.8664\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.1198 - acc: 0.9630 - val_loss: 0.5814 - val_acc: 0.8667\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.1173 - acc: 0.9633 - val_loss: 0.5859 - val_acc: 0.8662\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.1140 - acc: 0.9647 - val_loss: 0.5894 - val_acc: 0.8650\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.1120 - acc: 0.9651 - val_loss: 0.5996 - val_acc: 0.8654\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.1093 - acc: 0.9660 - val_loss: 0.6062 - val_acc: 0.8656\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.1065 - acc: 0.9670 - val_loss: 0.6082 - val_acc: 0.8653\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.1044 - acc: 0.9671 - val_loss: 0.6128 - val_acc: 0.8664\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.1024 - acc: 0.9679 - val_loss: 0.6171 - val_acc: 0.8650\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0995 - acc: 0.9690 - val_loss: 0.6264 - val_acc: 0.8647\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0974 - acc: 0.9695 - val_loss: 0.6240 - val_acc: 0.8650\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0958 - acc: 0.9697 - val_loss: 0.6317 - val_acc: 0.8652\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0934 - acc: 0.9707 - val_loss: 0.6398 - val_acc: 0.8641\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0911 - acc: 0.9715 - val_loss: 0.6434 - val_acc: 0.8643\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0898 - acc: 0.9717 - val_loss: 0.6454 - val_acc: 0.8650\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0873 - acc: 0.9722 - val_loss: 0.6497 - val_acc: 0.8650\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0860 - acc: 0.9727 - val_loss: 0.6505 - val_acc: 0.8656\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0836 - acc: 0.9737 - val_loss: 0.6646 - val_acc: 0.8641\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0824 - acc: 0.9738 - val_loss: 0.6710 - val_acc: 0.8652\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0805 - acc: 0.9744 - val_loss: 0.6706 - val_acc: 0.8649\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0786 - acc: 0.9750 - val_loss: 0.6786 - val_acc: 0.8648\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0772 - acc: 0.9751 - val_loss: 0.6809 - val_acc: 0.8647\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0754 - acc: 0.9759 - val_loss: 0.6855 - val_acc: 0.8644\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0741 - acc: 0.9763 - val_loss: 0.6900 - val_acc: 0.8645\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 60s 8ms/sample - loss: 0.0728 - acc: 0.9767 - val_loss: 0.6991 - val_acc: 0.8641\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0712 - acc: 0.9770 - val_loss: 0.7006 - val_acc: 0.8627\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0698 - acc: 0.9775 - val_loss: 0.6992 - val_acc: 0.8642\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0686 - acc: 0.9778 - val_loss: 0.7120 - val_acc: 0.8627\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 56s 7ms/sample - loss: 0.0671 - acc: 0.9783 - val_loss: 0.7154 - val_acc: 0.8637\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0658 - acc: 0.9786 - val_loss: 0.7127 - val_acc: 0.8634\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 56s 7ms/sample - loss: 0.0647 - acc: 0.9790 - val_loss: 0.7174 - val_acc: 0.8638\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0634 - acc: 0.9793 - val_loss: 0.7305 - val_acc: 0.8634\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 56s 7ms/sample - loss: 0.0622 - acc: 0.9795 - val_loss: 0.7253 - val_acc: 0.8634\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0610 - acc: 0.9800 - val_loss: 0.7319 - val_acc: 0.8636\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0599 - acc: 0.9804 - val_loss: 0.7394 - val_acc: 0.8635\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0588 - acc: 0.9807 - val_loss: 0.7383 - val_acc: 0.8629\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0575 - acc: 0.9810 - val_loss: 0.7452 - val_acc: 0.8636\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 55s 7ms/sample - loss: 0.0571 - acc: 0.9810 - val_loss: 0.7501 - val_acc: 0.8628\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 59s 7ms/sample - loss: 0.0561 - acc: 0.9814 - val_loss: 0.7548 - val_acc: 0.8629\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 56s 7ms/sample - loss: 0.0551 - acc: 0.9816 - val_loss: 0.7610 - val_acc: 0.8627\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 58s 7ms/sample - loss: 0.0542 - acc: 0.9818 - val_loss: 0.7550 - val_acc: 0.8633\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 58s 7ms/sample - loss: 0.0532 - acc: 0.9821 - val_loss: 0.7652 - val_acc: 0.8632\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 59s 7ms/sample - loss: 0.0526 - acc: 0.9824 - val_loss: 0.7704 - val_acc: 0.8629\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 59s 7ms/sample - loss: 0.0512 - acc: 0.9828 - val_loss: 0.7762 - val_acc: 0.8622\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 59s 7ms/sample - loss: 0.0509 - acc: 0.9830 - val_loss: 0.7864 - val_acc: 0.8627\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 56s 7ms/sample - loss: 0.0499 - acc: 0.9831 - val_loss: 0.7747 - val_acc: 0.8624\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 58s 7ms/sample - loss: 0.0491 - acc: 0.9835 - val_loss: 0.7813 - val_acc: 0.8632\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 57s 7ms/sample - loss: 0.0486 - acc: 0.9835 - val_loss: 0.7811 - val_acc: 0.8638\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 57s 7ms/sample - loss: 0.0477 - acc: 0.9837 - val_loss: 0.7870 - val_acc: 0.8622\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 58s 7ms/sample - loss: 0.0468 - acc: 0.9841 - val_loss: 0.7927 - val_acc: 0.8627\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 57s 7ms/sample - loss: 0.0463 - acc: 0.9841 - val_loss: 0.7945 - val_acc: 0.8632\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 56s 7ms/sample - loss: 0.0458 - acc: 0.9842 - val_loss: 0.7982 - val_acc: 0.8626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a396b82ef0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define the model that will turn\n",
    "#\"encoder_input_data\" &\"decoder_input_data\" into \"decoder_target_data\"\n",
    "model=Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
    "\n",
    "#Run training\n",
    "model.compile(optimizer=\"rmsprop\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "model.fit([encoder_input_data,decoder_input_data],decoder_target_data,\n",
    "         batch_size=batch_size,\n",
    "         epochs=epochs,\n",
    "         validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7a430d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input Sentence: Go.\n",
      "Decoded Sentence: Va !\n",
      "\n",
      "-\n",
      "Input Sentence: Run!\n",
      "Decoded Sentence: Cours !\n",
      "\n",
      "-\n",
      "Input Sentence: Run!\n",
      "Decoded Sentence: Cours !\n",
      "\n",
      "-\n",
      "Input Sentence: Wow!\n",
      "Decoded Sentence: Ça alors !\n",
      "\n",
      "-\n",
      "Input Sentence: Fire!\n",
      "Decoded Sentence: Au feu !\n",
      "\n",
      "-\n",
      "Input Sentence: Help!\n",
      "Decoded Sentence: À l'aide !\n",
      "\n",
      "-\n",
      "Input Sentence: Jump.\n",
      "Decoded Sentence: Saute.\n",
      "\n",
      "-\n",
      "Input Sentence: Stop!\n",
      "Decoded Sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input Sentence: Stop!\n",
      "Decoded Sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input Sentence: Stop!\n",
      "Decoded Sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input Sentence: Wait!\n",
      "Decoded Sentence: Attendez !\n",
      "\n",
      "-\n",
      "Input Sentence: Wait!\n",
      "Decoded Sentence: Attendez !\n",
      "\n",
      "-\n",
      "Input Sentence: Go on.\n",
      "Decoded Sentence: Poursuis.\n",
      "\n",
      "-\n",
      "Input Sentence: Go on.\n",
      "Decoded Sentence: Poursuis.\n",
      "\n",
      "-\n",
      "Input Sentence: Go on.\n",
      "Decoded Sentence: Poursuis.\n",
      "\n",
      "-\n",
      "Input Sentence: I see.\n",
      "Decoded Sentence: Je comprends.\n",
      "\n",
      "-\n",
      "Input Sentence: I try.\n",
      "Decoded Sentence: J'essaye.\n",
      "\n",
      "-\n",
      "Input Sentence: I won!\n",
      "Decoded Sentence: Je l'ai emporté !\n",
      "\n",
      "-\n",
      "Input Sentence: I won!\n",
      "Decoded Sentence: Je l'ai emporté !\n",
      "\n",
      "-\n",
      "Input Sentence: Oh no!\n",
      "Decoded Sentence: Oh non !\n",
      "\n",
      "-\n",
      "Input Sentence: Attack!\n",
      "Decoded Sentence: Attaque !\n",
      "\n",
      "-\n",
      "Input Sentence: Attack!\n",
      "Decoded Sentence: Attaque !\n",
      "\n",
      "-\n",
      "Input Sentence: Cheers!\n",
      "Decoded Sentence: Santé !\n",
      "\n",
      "-\n",
      "Input Sentence: Cheers!\n",
      "Decoded Sentence: Santé !\n",
      "\n",
      "-\n",
      "Input Sentence: Cheers!\n",
      "Decoded Sentence: Santé !\n",
      "\n",
      "-\n",
      "Input Sentence: Cheers!\n",
      "Decoded Sentence: Santé !\n",
      "\n",
      "-\n",
      "Input Sentence: Get up.\n",
      "Decoded Sentence: Lève-toi.\n",
      "\n",
      "-\n",
      "Input Sentence: Go now.\n",
      "Decoded Sentence: Va du fimi !\n",
      "\n",
      "-\n",
      "Input Sentence: Go now.\n",
      "Decoded Sentence: Va du fimi !\n",
      "\n",
      "-\n",
      "Input Sentence: Go now.\n",
      "Decoded Sentence: Va du fimi !\n",
      "\n",
      "-\n",
      "Input Sentence: Got it!\n",
      "Decoded Sentence: Compris !\n",
      "\n",
      "-\n",
      "Input Sentence: Got it!\n",
      "Decoded Sentence: Compris !\n",
      "\n",
      "-\n",
      "Input Sentence: Got it?\n",
      "Decoded Sentence: Pigé ?\n",
      "\n",
      "-\n",
      "Input Sentence: Got it?\n",
      "Decoded Sentence: Pigé ?\n",
      "\n",
      "-\n",
      "Input Sentence: Got it?\n",
      "Decoded Sentence: Pigé ?\n",
      "\n",
      "-\n",
      "Input Sentence: Hop in.\n",
      "Decoded Sentence: Monte.\n",
      "\n",
      "-\n",
      "Input Sentence: Hop in.\n",
      "Decoded Sentence: Monte.\n",
      "\n",
      "-\n",
      "Input Sentence: Hug me.\n",
      "Decoded Sentence: Serrez-moi dans vos bras !\n",
      "\n",
      "-\n",
      "Input Sentence: Hug me.\n",
      "Decoded Sentence: Serrez-moi dans vos bras !\n",
      "\n",
      "-\n",
      "Input Sentence: I fell.\n",
      "Decoded Sentence: Je suis tombé.\n",
      "\n",
      "-\n",
      "Input Sentence: I fell.\n",
      "Decoded Sentence: Je suis tombé.\n",
      "\n",
      "-\n",
      "Input Sentence: I know.\n",
      "Decoded Sentence: Je sais.\n",
      "\n",
      "-\n",
      "Input Sentence: I left.\n",
      "Decoded Sentence: Je suis parti.\n",
      "\n",
      "-\n",
      "Input Sentence: I left.\n",
      "Decoded Sentence: Je suis parti.\n",
      "\n",
      "-\n",
      "Input Sentence: I lost.\n",
      "Decoded Sentence: J'ai perdu.\n",
      "\n",
      "-\n",
      "Input Sentence: I'm 19.\n",
      "Decoded Sentence: J'ai écapté.\n",
      "\n",
      "-\n",
      "Input Sentence: I'm OK.\n",
      "Decoded Sentence: Je vais bien.\n",
      "\n",
      "-\n",
      "Input Sentence: I'm OK.\n",
      "Decoded Sentence: Je vais bien.\n",
      "\n",
      "-\n",
      "Input Sentence: Listen.\n",
      "Decoded Sentence: Écoutez !\n",
      "\n",
      "-\n",
      "Input Sentence: No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence: No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence: No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence: No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence: No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence: No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence: No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence: No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence: No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input Sentence: Really?\n",
      "Decoded Sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input Sentence: Really?\n",
      "Decoded Sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input Sentence: Really?\n",
      "Decoded Sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input Sentence: Thanks.\n",
      "Decoded Sentence: Merci !\n",
      "\n",
      "-\n",
      "Input Sentence: We try.\n",
      "Decoded Sentence: On essaye.\n",
      "\n",
      "-\n",
      "Input Sentence: We won.\n",
      "Decoded Sentence: Nous avons gagné.\n",
      "\n",
      "-\n",
      "Input Sentence: We won.\n",
      "Decoded Sentence: Nous avons gagné.\n",
      "\n",
      "-\n",
      "Input Sentence: We won.\n",
      "Decoded Sentence: Nous avons gagné.\n",
      "\n",
      "-\n",
      "Input Sentence: We won.\n",
      "Decoded Sentence: Nous avons gagné.\n",
      "\n",
      "-\n",
      "Input Sentence: Ask Tom.\n",
      "Decoded Sentence: Demande à Tom.\n",
      "\n",
      "-\n",
      "Input Sentence: Awesome!\n",
      "Decoded Sentence: Fantastique !\n",
      "\n",
      "-\n",
      "Input Sentence: Be calm.\n",
      "Decoded Sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input Sentence: Be calm.\n",
      "Decoded Sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input Sentence: Be calm.\n",
      "Decoded Sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input Sentence: Be cool.\n",
      "Decoded Sentence: Sois détéle !\n",
      "\n",
      "-\n",
      "Input Sentence: Be fair.\n",
      "Decoded Sentence: Sois équitable !\n",
      "\n",
      "-\n",
      "Input Sentence: Be fair.\n",
      "Decoded Sentence: Sois équitable !\n",
      "\n",
      "-\n",
      "Input Sentence: Be fair.\n",
      "Decoded Sentence: Sois équitable !\n",
      "\n",
      "-\n",
      "Input Sentence: Be fair.\n",
      "Decoded Sentence: Sois équitable !\n",
      "\n",
      "-\n",
      "Input Sentence: Be fair.\n",
      "Decoded Sentence: Sois équitable !\n",
      "\n",
      "-\n",
      "Input Sentence: Be fair.\n",
      "Decoded Sentence: Sois équitable !\n",
      "\n",
      "-\n",
      "Input Sentence: Be kind.\n",
      "Decoded Sentence: Sois gentil.\n",
      "\n",
      "-\n",
      "Input Sentence: Be nice.\n",
      "Decoded Sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input Sentence: Be nice.\n",
      "Decoded Sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input Sentence: Be nice.\n",
      "Decoded Sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input Sentence: Be nice.\n",
      "Decoded Sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input Sentence: Be nice.\n",
      "Decoded Sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input Sentence: Be nice.\n",
      "Decoded Sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input Sentence: Beat it.\n",
      "Decoded Sentence: Dégage !\n",
      "\n",
      "-\n",
      "Input Sentence: Call me.\n",
      "Decoded Sentence: Appellez-moi !\n",
      "\n",
      "-\n",
      "Input Sentence: Call me.\n",
      "Decoded Sentence: Appellez-moi !\n",
      "\n",
      "-\n",
      "Input Sentence: Call us.\n",
      "Decoded Sentence: Appelle-nous !\n",
      "\n",
      "-\n",
      "Input Sentence: Call us.\n",
      "Decoded Sentence: Appelle-nous !\n",
      "\n",
      "-\n",
      "Input Sentence: Come in.\n",
      "Decoded Sentence: Entrez !\n",
      "\n",
      "-\n",
      "Input Sentence: Come in.\n",
      "Decoded Sentence: Entrez !\n",
      "\n",
      "-\n",
      "Input Sentence: Come in.\n",
      "Decoded Sentence: Entrez !\n",
      "\n",
      "-\n",
      "Input Sentence: Come in.\n",
      "Decoded Sentence: Entrez !\n",
      "\n",
      "-\n",
      "Input Sentence: Come on!\n",
      "Decoded Sentence: Allez !\n",
      "\n",
      "-\n",
      "Input Sentence: Come on.\n",
      "Decoded Sentence: Venez !\n",
      "\n",
      "-\n",
      "Input Sentence: Come on.\n",
      "Decoded Sentence: Venez !\n",
      "\n",
      "-\n",
      "Input Sentence: Come on.\n",
      "Decoded Sentence: Venez !\n",
      "\n",
      "-\n",
      "Input Sentence: Drop it!\n",
      "Decoded Sentence: Laissez tomber !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Next:inference mode(sampling)\n",
    "#Here's the drill:\n",
    "#1) encode input and retrieve initial decoder state\n",
    "#2) run one step of decoder with this initial state\n",
    "#and a \"start of sequence\" taken as target.\n",
    "#output will be a next target token\n",
    "#3)Repeat with the current target taken and current states\n",
    "\n",
    "#Define sampling models\n",
    "encoder_model=Model(encoder_inputs,encoder_states)\n",
    "\n",
    "decoder_state_inputs_h=Input(shape=(latent_dim,))\n",
    "decoder_state_inputs_c=Input(shape=(latent_dim,))\n",
    "decoder_state_inputs=[decoder_state_inputs_h,decoder_state_inputs_c]\n",
    "decoder_outputs,state_h,state_c=decoder_lstm(\n",
    "    decoder_inputs,initial_state=decoder_state_inputs)\n",
    "decoder_states=[state_h,state_c]\n",
    "decoder_outputs=decoder_dense(decoder_outputs)\n",
    "decoder_model=Model(\n",
    "    [decoder_inputs]+decoder_state_inputs,\n",
    "    [decoder_outputs]+decoder_states)\n",
    "\n",
    "#Reverse-lookup taken index to decode sequences back to\n",
    "#something readable\n",
    "reverse_input_char_index=dict(\n",
    "    (i,char) for char,i in input_token_index.items())\n",
    "reverse_target_char_index=dict(\n",
    "    (i,char) for char,i in target_token_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    #Encode the input as state vector\n",
    "    states_value=encoder_model.predict(input_seq)\n",
    "    \n",
    "    #Generate empty target sequence of length 1.\n",
    "    target_seq=np.zeros((1,1,num_decoder_tokens))\n",
    "    #Populate the first character of target sequence with the start character.\n",
    "    target_seq[0,0,target_token_index[\"\\t\"]]=1.\n",
    "    \n",
    "    #sampling loop for a batch of sequences\n",
    "    #(to simplify here we assume a batch of size 1).\n",
    "    stop_condition=False\n",
    "    decoded_sentence=\"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens,h,c= decoder_model.predict(\n",
    "            [target_seq]+states_value)\n",
    "        \n",
    "        #sample a token\n",
    "        sampled_token_index=np.argmax(output_tokens[0,-1,:])\n",
    "        sampled_char=reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence+=sampled_char\n",
    "        \n",
    "        #Exit condition:either hit max length\n",
    "        #or find stop character.\n",
    "        if (sampled_char==\"\\n\"or\n",
    "           len(decoded_sentence)>max_decoder_seq_length):\n",
    "            stop_condition=True\n",
    "            \n",
    "        #update the target sequence (of lenght 1).\n",
    "        target_seq=np.zeros((1,1,num_decoder_tokens))\n",
    "        target_seq[0,0,sampled_token_index]=1.\n",
    "        \n",
    "        #update states\n",
    "        states_value=[h,c]\n",
    "        \n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    #Take one sequence (part of the training set)\n",
    "    #for trying out decoding.\n",
    "    input_seq=encoder_input_data[seq_index:seq_index +1]\n",
    "    decoded_sentence=decode_sequence(input_seq)\n",
    "    print(\"-\")\n",
    "    print(\"Input Sentence:\",input_texts[seq_index])\n",
    "    print(\"Decoded Sentence:\",decoded_sentence)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f096fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
